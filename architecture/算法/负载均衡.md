# 负载均衡
## 分类
- 静态负载均衡算法
- 动态负载均衡算法
## 常见算法
- 轮询与加权轮询
  - 平滑的加权轮询算法
    - 平滑的加权轮训算法，最早是在Nginx中出现，https://github.com/phusion/nginx/commit/27e94984486058d73157038f7950a0a36ecc6e35
- 随机与加权随机
  - 轮询算法和随机算法，从统计学角度来看，最终效果是一样的。但是轮询算法天然的就会比随机算法更平滑，可以避免连读多次请求打到一个节点上。
- 哈希与一致性哈希
  - 哈希算法的选取会严重影响负载均衡的效果。假如说你计算哈希值的算法不太好，就容易导致某几个节点上负载特别高，而其他节点的负载就比较低。所以要尽可能保证哈希值计算出来的结果是均匀的。
  - 一致性哈希负载均衡引入了一个哈希环的概念，服务端节点会落在环的某些位置上。客户端根据请求参数，计算一个哈希值。这个哈希值会落在哈希环的某个位置。从这个位置出发，顺时针查找，遇到的第一个服务端节点就是目标节点。
  - 普通哈希会面临一个问题，就是当增加或删除节点时，哈希值会重新落在不同的节点上，违背使用哈希算法的本意。
  - 一致性的意思是保证当服务集群某个真实服务器出现故障，只影响该服务器的哈希，而不会导致整个服务集群的哈希键值重新分布。
- 最少连接数
  - 如果一个服务端节点上的连接数越多，那么这个节点的负载就越高。因此在做负载均衡的时候就是看一下客户端和各个节点的连接数量，从中挑选出连接数数量最少的节点。
  - 接数并不能代表节点的实际负载，尤其是在连接多路复用的情况下。
  - 在基本算法总结里面我用最少连接数算法举一个反面例子，但是同样的算法用在网关负载均衡上，就没有类似的问题，为什么？ 
    - 客户端统计的连接数只是客户端自己与服务端之间的连接数，并不能代表服务端上所有的连接数，所以不具备参考性。而网关是服务端所有连接的入口，网关上统计的连接数实际上就是服务端的所有连接数，所以这个指标是有参考性的。
- 最少活跃数
  - 最少活跃数算法则是用当前活跃请求数来代表服务端节点的负载。
  - 活跃请求数量也不能真正代表服务端节点的负载
    - 服务端节点 1 虽然只有 10 个请求，但是万一这 10 个请求都是大请求，例如大商家、大买家或者千万粉丝 UP 主的请求，那么服务端节点 1 的负载也会显著高于其他两个节点。
- 最快响应时间
  - 用的是响应时间来代表服务端节点的负载。
  - 最快响应时间算法就是客户端维持每个节点的响应时间，而后每次挑选响应时间最短的。
  - 注意响应时间的时效性。一般来说统计响应时间时应该只用近期请求的响应时间，并且越近的响应时间，权重应该越高。换句话说，就是采集的响应时间效用应该随着时间衰减。
- 总结：
  - 最少连接数、最少活跃数、最快响应时间都是客户端来采集数据的。那么不同的客户端就可能采集到不同的数据。
    - 如何解决呢?让服务端上报指标，而不是客户端采集
      - 服务端在返回响应的时候顺便把服务端上的一些信息一并返回。这种思路需要微服务框架支持从服务端往客户端回传链路元数据。
      - 从观测平台上查询。例如通过查询 Prometheus 来获得各种指标数据。
    - 不过目前业界很少用这种复杂的负载均衡算法，也因此几乎所有的微服务框架都没有服务端上报指标到客户端的机制。

## Q&A
- 公司有 Nginx 之类的网关，或者微服务网关，那么用的是什么负载均衡算法？
- 公司用客户端负载均衡的话，用的是什么负载均衡算法？
- 有没有出过和负载均衡相关的事故，如果有，那么是什么原因导致的，怎么解决的这个事故，它体现了负载均衡算法的什么缺陷？
- 怎么根据调用结果来调整权重，从而影响负载均衡的效果？
  - 加权类的算法都要考虑权重的设置和调整。（成加败减）
  - 加权类的负载均衡算法都会考虑根据调用结果来动态调整权重。如果调用成功了，那么就增加权重；如果调用失败了，那么就减少权重。这里调用成功与否是一种非业务相关的概念，也就是说即便拿到了一个失败的响应，但是本身也算是调用成功了。调用失败了大多数时候是指网络错误、超时等。而在实际落地的时候，也可以考虑如果是网络引起的失败，那么权重下调就多一点，因为这一类的错误意味着问题更加严重。如果是超时这种，那么权重就下调少一点，因为这种错误是比较容易恢复过来的。
  - 权重的调整要设置好上限和下限
    - 调整权重的算法都要考虑安全问题，即权重的调整应该有上限和下限。比如说一般下限不能为 0，因为一个节点的权重为 0 的话，它可能永远也不会被选中，又或者和 0 的数学运算会出现问题导致负载均衡失败。上限一般不超过初始权重的几倍，比如说两倍或者三倍，防止该节点一直被连续选中。
  - 这种根据调用结果来调整权重的方式，有点类似于在服务中将暂时调用不通的节点挪出可用节点列表，本质上都是为了进一步提高系统的可用性。
- 怎么利用一致性哈希负载均衡算法，来提高本地缓存命中率，缓解数据不一致性问题？
  - 使用本地缓存，那么同一个 key 对应的请求，可能会被打到不同的节点上。这就会造成两个问题，一个是严重的缓存未命中，一个是不同节点都要缓存同样的数据，导致内存浪费和极其严重的数据一致性问题。
  - 所以在这种情况下，一个很自然的想法就是能不能把类似的请求都让同一个节点来处理。比如说对某个用户数据的请求都打到同一个节点上。
  - 在性能非常苛刻的时候，我们会考虑使用本地缓存。但是使用本地缓存的数据一致性问题会非常严重，而我们可以尝试将一致性哈希负载均衡算法和本地缓存结合在一起，以提高缓存命中率，并且降低本地缓存的总体内存消耗。比如说针对用户的本地缓存，我们可以使用用户 ID 来计算哈希值，那么可以确保同一个用户的本地缓存必然在同一个节点上。不过即便是采用了一致性哈希负载均衡算法，依旧不能彻底解决数据一致性的问题，只能缓解一下。
  - 当整个集群的节点数量发生变化的时候，就难免会导致同样的数据缓存在多个节点上。
  - 例如在用户这个例子中，假如最开始有一个请求需要 user_id 为 1 的昵称小明，这个请求最开始会命中老节点。但是此时还没有查询到数据。紧接着扩容。此时又来了一个请求，那么它会被导去新节点。这一个请求会将 user_id 为 1 的昵称改为小刚。如果这时候第一个请求从老节点的缓存上读出了数据，那么它拿到的就还是老的数据。而应用发布是引起节点数量变化最常见的原因。毕竟应用发布可以看作先下线一个节点，再上线一个节点。
  - 不过同时也可以看出来，在本地缓存结合了一致性哈希负载均衡算法之后数据一致性的问题已经被大大缓解了。


## 案例
- 1、公司用的是轮询来作为负载均衡。不过因为轮询没有实际查询服务端节点的负载，所以难免会出现偶发性的负载不均衡的问题。 之前发现线上的响应时间总体来说是非常均匀的，但是每隔一段时间就会出现响应时间特别慢的情况。而且时间间隔是不固定的，慢的程度也不一样，所以就很奇怪。后来我们经过排查之后，发现是因为当一个大请求落到一个节点的时候，它会占据大量的内存和 CPU。如果这时候再有请求打到同一个节点上，这部分请求的响应时间就会非常慢。
  - 例子说明了所有负载均衡算法都有的缺点，即没有考虑请求本身。（大请求）
  - 解法
    - 业务拆分：这个大请求其实是一个大的批量请求。后来我们限制一批最多只能取 100 个就解决了这个问题。
    - 隔离角度：稍微魔改了一下负载均衡算法，不再是单纯的轮询了。我们每天计算一批大客户，这部分大客户的请求会在负载均衡里面被打到专门的几个节点上。虽然大客户的请求依旧很慢，但是至少别的客户不会再受到他们的影响了。
- 负载均衡算法有些时候用得好，是能够解决一些技术问题的，比如说缓存。（钓鱼）